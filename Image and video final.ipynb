{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of video-final",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyankaVerma98/CompArch-LAbs/blob/master/Image%20and%20video%20final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON43dWVCD5GM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "directory = '/content/drive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9SSO1yfDfwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5914EhmREM1s",
        "colab_type": "code",
        "outputId": "8d207328-8773-425c-c6ff-b93fc2461d7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVOhmV_vEjOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzMhR8DEEoCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9xWPc7BEs1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageDirectory = directory + 'image/Flicker8k_Dataset/'\n",
        "textDirectory = directory + 'text'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHApr1ebEx1o",
        "colab_type": "text"
      },
      "source": [
        "# ***Training the Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tln7azbeHYvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions\n",
        "\n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t# load all features\n",
        "\tall_features = load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features\n",
        "\n",
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)\n",
        "\n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each description for the image\n",
        "\tfor desc in desc_list:\n",
        "\t\t# encode the sequence\n",
        "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t# split one sequence into multiple X,y pairs\n",
        "\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t# split into input and output pair\n",
        "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t# pad input sequence\n",
        "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t# encode output sequence\n",
        "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t# store\n",
        "\t\t\tX1.append(photo)\n",
        "\t\t\tX2.append(in_seq)\n",
        "\t\t\ty.append(out_seq)\n",
        "\treturn array(X1), array(X2), array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWnZ72RZDc6k",
        "colab_type": "code",
        "outputId": "98d4bb4b-cb5a-49d6-88c7-1302449e25d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "\t# feature extractor model\n",
        "\tinputs1 = Input(shape=(4096,))\n",
        "\tfe1 = Dropout(0.5)(inputs1)\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\n",
        "\t# sequence model\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\tse3 = LSTM(256)(se2)\n",
        "\t# decoder model\n",
        "\tdecoder1 = add([fe2, se3])\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\t# tie it together [image, seq] [word]\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\t# compile model\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\t# summarize model\n",
        "\tmodel.summary()\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model\n",
        "\n",
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
        "\t# loop for ever over images\n",
        "\twhile 1:\n",
        "\t\tfor key, desc_list in descriptions.items():\n",
        "\t\t\t# retrieve the photo feature\n",
        "\t\t\tphoto = photos[key][0]\n",
        "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
        "\t\t\tyield [[in_img, in_seq], out_word]\n",
        "\n",
        "# load training dataset (6K)\n",
        "filename = directory + 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions(directory+'descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "\n",
        "\n",
        "\n",
        "# define the model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# train the model, run epochs manually and save after each epoch\n",
        "epochs = 25\n",
        "steps = len(train_descriptions)\n",
        "for i in range(epochs):\n",
        "\t# create the data generator\n",
        "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
        "\t# fit for one epoch\n",
        "\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "\t# save model\n",
        "\tif(i%5==0) :\n",
        "\t\tmodel.save(directory + 'model_' + str(i) + '.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 34)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 34, 256)      1940224     input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 4096)         0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 34, 256)      0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 256)          1048832     dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 256)          525312      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 256)          0           dense_7[0][0]                    \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 256)          65792       add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 7579)         1947803     dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,527,963\n",
            "Trainable params: 5,527,963\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 421s 70ms/step - loss: 4.6838\n",
            "Epoch 1/1\n",
            "3776/6000 [=================>............] - ETA: 2:35 - loss: 3.9499"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-250b44718317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# fit for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iUvU1e5rv8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(directory + 'model_' + str(i) + '.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4DoSvQaGrI-",
        "colab_type": "text"
      },
      "source": [
        "# *EVALUATING THE MODEL* \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE4dc842GsAW",
        "colab_type": "code",
        "outputId": "ba7ee0e1-e4c2-443e-cdec-5c1486864a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from numpy import argmax\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HafkBXlf5MVs",
        "colab_type": "code",
        "outputId": "bdfe7ce4-2fc0-4458-8783-93e828d33302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# prepare test set\n",
        " \n",
        "# load test set\n",
        "filename = directory + 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions(directory+ 'descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features(directory + 'features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y87CiTbdIF6S",
        "colab_type": "code",
        "outputId": "e74ee76f-dd86-47e5-960b-2f6a401d97f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from numpy import argmax\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions\n",
        "\n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t# load all features\n",
        "\tall_features = load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features\n",
        "\n",
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\t# seed the generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# iterate over the whole length of the sequence\n",
        "\tfor i in range(max_length):\n",
        "\t\t# integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\t# predict next word\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\t# convert probability to integer\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\t# map integer to word\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\t# stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# prepare tokenizer on train set\n",
        "\n",
        "# load training dataset (6K)\n",
        "filename = directory+ 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions( directory + 'descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)\n",
        "\n",
        "# prepare test set\n",
        "\n",
        "# load test set\n",
        "filename = directory + 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions(directory + 'descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features(directory +'features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n",
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbf2hyfIcbd8",
        "colab_type": "code",
        "outputId": "26a8bba2-929d-47ba-c8e2-a6f50d091d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "filename = 'model_9.h5'\n",
        "model = load_model(filename)\n",
        "# evaluate model\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.510517\n",
            "BLEU-2: 0.258302\n",
            "BLEU-3: 0.173642\n",
            "BLEU-4: 0.076306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Ji8io2HrZ_",
        "colab_type": "code",
        "outputId": "d1f8ef2c-4558-456a-927f-395116f8d939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "filename = directory + 'model_20.h5'\n",
        "model = load_model(filename)\n",
        "# evaluate model\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.508636\n",
            "BLEU-2: 0.262012\n",
            "BLEU-3: 0.176529\n",
            "BLEU-4: 0.077944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCEOx1zmOkzr",
        "colab_type": "text"
      },
      "source": [
        "# Captioning random inages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxXlN-LsOj9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_json\n",
        "import numpy\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs1FhqpdPif6",
        "colab_type": "code",
        "outputId": "420ad8c4-17d9-4f54-a740-bf574cfc88dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(directory + \"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(directory + \"weightsaved.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIt-65z1OI18",
        "colab_type": "code",
        "outputId": "1acec106-734d-4432-8567-c12b88d3ecc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "json_file = open(directory + 'model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(directory + \"weightsaved.h5\")\n",
        "print(\"Loaded model from disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oShNbfKQ7B2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(filename):\n",
        "\t# load the model\n",
        "\tmodel = VGG16()\n",
        "\t# re-structure the model\n",
        "\tmodel.layers.pop()\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\t# load the photo\n",
        "\timage = load_img(filename, target_size=(224, 224))\n",
        "\t# convert the image pixels to a numpy array\n",
        "\timage = img_to_array(image)\n",
        "\t# reshape data for the model\n",
        "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t# prepare the image for the VGG model\n",
        "\timage = preprocess_input(image)\n",
        "\t# get features\n",
        "\tfeature = model.predict(image, verbose=0)\n",
        "\treturn feature\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G8PvUz5RdjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from pickle import dump"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiRFp2ijRxSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import VGG16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u_R8zrJUtzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pickle import dump\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voyAPNnjTMSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWzDwBFzOOS9",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://patcegan.files.wordpress.com/2013/12/child-with-lollipop-licensed.jpg\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcQwxJ0zgBrk",
        "colab_type": "text"
      },
      "source": [
        "# ***OUTPUT***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyEb1pAcQkBf",
        "colab_type": "code",
        "outputId": "71c28192-ecc7-4361-dcf8-8f40ea64a7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "photo = extract_features(directory +'happychild.jpg')\n",
        "\n",
        "description = generate_desc(loaded_model, tokenizer, photo, max_length)\n",
        "print(description)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq baby with blonde hair is eating snack endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQCPBqO1U8FF",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/example.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNBxQ7tOf3XK",
        "colab_type": "text"
      },
      "source": [
        "# ***OUTPUT***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5NSGEnjQsJA",
        "colab_type": "code",
        "outputId": "e21495cb-ef53-4ebe-9f1f-17b63ab0581f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "photo = extract_features(directory +'example.jpg')\n",
        "\n",
        "description = generate_desc(loaded_model, tokenizer, photo, max_length)\n",
        "print(description)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq two dogs are running through the water endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gAF0FhTVQYE",
        "colab_type": "text"
      },
      "source": [
        "# ***Videos***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5CgEGklVPxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import imutils\n",
        "import glob\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciRUZ82KVUrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap = cv2.VideoCapture(directory + \"vid1check.mp4\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9Aot1e6VcC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pathOut = r\"/content/drive/My Drive/Colab Notebooks/\"\n",
        "count = 0\n",
        "counter = 1\n",
        "\n",
        "# vid = directory + \"vid1check.mp4\" +vid\n",
        "# cap = cv2.VideoCapture(vid)\n",
        "count = 0\n",
        "counter += 1\n",
        "success = True\n",
        "total = []\n",
        "while success:\n",
        "    success,image = cap.read()\n",
        "    if count%200 == 0 :\n",
        "          cv2.imwrite(pathOut + 'newframe%d.jpg'%count,image)\n",
        "          photo = extract_features(directory + \"newframe%d.jpg\"%count)\n",
        "          description = generate_desc(loaded_model, tokenizer, photo, max_length)\n",
        "          print(description)\n",
        "          total.append(description)\n",
        "    count+=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yjs4YC3QgPSD",
        "colab_type": "text"
      },
      "source": [
        "***OUTPUT***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99SiLXezW8pI",
        "colab_type": "code",
        "outputId": "8cc4ef67-0f9c-491f-c8d6-0966d2ff336b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(total)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['startseq man in black shirt is standing in front of the street endseq', 'startseq two children are playing with soccer ball in match endseq', 'startseq two children are playing with soccer ball in match endseq', 'startseq man in red shirt is standing in front of the street endseq', 'startseq man in red shirt is standing in front of the road endseq']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}